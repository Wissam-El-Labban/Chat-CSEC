{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import keys\n",
    "\n",
    "OPENAI_API_KEY = keys.OPENAI_API_KEY\n",
    "PINECONE_API_KEY = keys.PINECONE_API_KEY\n",
    "PINECONE_API_ENV = keys.PINECONE_API_ENV\n",
    "\n",
    "#there are many options from the above that can be used. Langchain offeres many libraries that allow users to interact with OpenAIs LLMS given that they have an API key \n",
    "#for this code however, the lines from embeddings, chat_models, memory, and qa chain are used\n",
    "# embeddings are used to convert the textual infroamtion in documents into vector data that the LLM can understand and query using the semantics in a users query to find relevant results\n",
    "# ChatOpenAI gives us the ability to choose which LLM we can use. The LLM will look into the vector databse and contruct an answer. We will use our fine tuned instance once we find a way to make it accessible throught this code\n",
    "# ChatMessage History allows use to build a history object where the LLM keeps track of user questions and AI responses to keep the conversation relevant\n",
    "# load_qa_chain is the question an answer chain that allows us to run the a user quer with the vector database. usually this function returns a generic response. but the code below has an input documents similarity parameter to get relevant information\n",
    "\n",
    "\n",
    "\n",
    "# always remove the API key before committing code to github. \n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# temperature in the function below is 0. this is the randomness variable. a temperature value > 1 will be completely random. \n",
    "# the value below is at 0 to have no randomness. this coupled with our relevant document search is our method of mitigating hallucinations from the LLM\n",
    "\n",
    "#llm = ChatOpenAI(model='gpt-3.5-turbo',temperature=0, openai_api_key=API_Key)\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "llm = ChatOpenAI(model=model_name,temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type='stuff')\n",
    "\n",
    "# initializing history to track the conversation\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "\n",
    "# the value below only shows the messages from the entire conversation history\n",
    "history.messages\n",
    "\n",
    "query = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our database value is vectordb. we are using the file db2 which has our pdfs and CSVs. we had db and db (csv) as test cases aginst information in pdfs and CSVs. \n",
    "# we did not know how a vector databse would behave if we gave put differently structued data in one place. but its working out well so far. \n",
    "\n",
    "vectordb = Chroma(persist_directory='db2', embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a retriever function to a value. this allows the program to retrieve documents for our similartiy search which matches the semantics of our query to info in the databse\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hey\n",
      "\n",
      "--------------------------\n",
      "debug mode start\n",
      "\n",
      "Simlar dosuments are: [Document(page_content='}\\n}\\n}\\n}\\nreturn\\n0;\\n}', metadata={}), Document(page_content='1NA NA NA NA NA NA\\n1NA NA NA NA NA NA\\n1NA NA NA NA NA NA', metadata={}), Document(page_content='obtained byfollowing\\n17', metadata={}), Document(page_content='instructions. ########', metadata={}), Document(page_content='instructions. ########', metadata={}), Document(page_content='instructions. ########', metadata={}), Document(page_content='instructions. ########', metadata={}), Document(page_content='instructions. ########', metadata={}), Document(page_content='16NA NA NA NA NA NA\\n16NA NA NA NA NA NA\\n16NA NA NA NA NA NA', metadata={}), Document(page_content='19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n19NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA\\n20NA NA NA NA NA NA', metadata={})]\n",
      "\n",
      "number of documents: 4\n",
      "\n",
      "debug mode end\n",
      "---------------------------\n",
      "\n",
      "Chat CSEC: Hello! How can I assist you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the below is our current chat backend. in later sprints we will have user interface. but the below is fine for our testing purposes\n",
    "# if the user types exit, the loop ends\n",
    "\n",
    "while True:\n",
    "    query = str(input(\"Human: \"))\n",
    "    if query == 'exit':\n",
    "        break\n",
    "    # adding a human message to history\n",
    "    history.add_user_message(query)\n",
    "    print(f'Human: {query}')\n",
    "    print()\n",
    "    validity = len(retriever.get_relevant_documents(query))\n",
    "    # the below if statment will probably never hit. semantics so far have never caused no documents to show up. \n",
    "    if validity == 0:\n",
    "        response = chat(history.messages)\n",
    "        print(\"question cannot be answered by database\")\n",
    "        print(response.content)\n",
    "        history.add_ai_message(response.content)\n",
    "        history.messages\n",
    "    # in the below else statement, we have a similarity value that will take semantics from the databse that match our query\n",
    "    # the chain.run statement from qa chain will generate a response using both similarity and quer to thier respective values in the function\n",
    "    else:\n",
    "        similarity = vectordb.similarity_search(query, k=10)\n",
    "        # uncomment the below to enable debug mode\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print('debug mode start')\n",
    "        print()\n",
    "        print(f'Simlar dosuments are: {similarity}')\n",
    "        print()\n",
    "        print(f'number of documents: {validity}')\n",
    "        print()\n",
    "        print('debug mode end')\n",
    "        print('---------------------------')\n",
    "        print()\n",
    "        \n",
    "        response = chain.run(input_documents=similarity, question=query)\n",
    "        print(f'Chat CSEC: {response}')\n",
    "        print()\n",
    "    # adding the response as the AI message to add to the chat history\n",
    "        history.add_ai_message(response)\n",
    "        history.messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatCSEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
