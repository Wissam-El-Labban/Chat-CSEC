{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "#importing from keys file\n",
    "import keys\n",
    "\n",
    "# in the above code, we integrated the API key as an os environment variable. this method of defining the API key for the program is optional\n",
    "# in the above libraries, Chroma, embeddings, text splitter are used. \n",
    "# embeddings are the vectors constructed from semantings in the documents that are used in order to query a result based on a similarity search. when we want to query information\n",
    "# for document loaders, I am using Py PDF loader.\n",
    "# chroma is a vector databse service. the vector databese in use is a chroma db one. \n",
    "# for the cloud based solution, we can either move the chroma db to a cloud platform, or use another service like oinecone or weviate. \n",
    "# chroma is highly scalable and open source and since we are using it it is perefered to keep its use. \n",
    "\n",
    "# this file will be used to add data to the vector databse. this includes CSVs. CSVs need to be convereted to PDFs before going into a database. \n",
    "# the CSV file notebook consists of the same code as this and can be ignored. I am keeping it around in case we want to load CSV type files into a different database directory.\n",
    "\n",
    "OPENAI_API_KEY = keys.OPENAI_API_KEY\n",
    "PINECONE_API_KEY = keys.PINECONE_API_KEY\n",
    "PINECONE_API_ENV = keys.PINECONE_API_ENV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path .pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Chat-CSEC\\Chromadb Input.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chat-CSEC/Chromadb%20Input.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# we will load the pdf file of choice into the loader vaiable\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chat-CSEC/Chromadb%20Input.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# we will load the data into a data variable\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chat-CSEC/Chromadb%20Input.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chat-CSEC/Chromadb%20Input.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#loader = PyPDFLoader('CrowdStrike2023GlobalThreatReport.pdf')\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Chat-CSEC/Chromadb%20Input.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m loader \u001b[39m=\u001b[39m PyPDFLoader(\u001b[39m'\u001b[39;49m\u001b[39m.pdf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chat-CSEC/Chromadb%20Input.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m data \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload()\n",
      "File \u001b[1;32mc:\\Users\\wissa\\anaconda3\\envs\\ChatCSEC\\lib\\site-packages\\langchain\\document_loaders\\pdf.py:151\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[1;34m(self, file_path, password)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    148\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpypdf package not found, please install it with \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m`pip install pypdf`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m     )\n\u001b[0;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparser \u001b[39m=\u001b[39m PyPDFParser(password\u001b[39m=\u001b[39mpassword)\n\u001b[1;32m--> 151\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(file_path)\n",
      "File \u001b[1;32mc:\\Users\\wissa\\anaconda3\\envs\\ChatCSEC\\lib\\site-packages\\langchain\\document_loaders\\pdf.py:97\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(temp_pdf)\n\u001b[0;32m     96\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path):\n\u001b[1;32m---> 97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFile path \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not a valid file or url\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path)\n",
      "\u001b[1;31mValueError\u001b[0m: File path .pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "# we will load the pdf file of choice into the loader vaiable\n",
    "# we will load the data into a data variable\n",
    "\n",
    "#loader = PyPDFLoader('CrowdStrike2023GlobalThreatReport.pdf')\n",
    "loader = PyPDFLoader('.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 307 document(s) in your data\n",
      "There are 181 characters in your document\n"
     ]
    }
   ],
   "source": [
    "# getting a view of how long our data is\n",
    "\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now you have 630 documents\n"
     ]
    }
   ],
   "source": [
    "# there is a limit to how much data we can store at once\n",
    "# the code below splits it into sizeable chuncks that can be loaded into the database one at a time\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)\n",
    "print(f'now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the database folder to store our vector data\n",
    "persist_directory = 'db2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an embeddings variable to make the vectors out of the text\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are creating a docsearch variable that will hold our data. it is also worth noting that in this process the vectors go into the databse we chose\n",
    "# docsearch.persist will persist the data in the databse. \n",
    "\n",
    "docsearch = Chroma.from_texts([t.page_content for t in texts], embeddings, persist_directory=persist_directory)\n",
    "docsearch.persist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatCSEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
